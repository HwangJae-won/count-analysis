import os, sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from logger import logger
from konlpy.tag import Komoran
import pandas as pd
from process import get_hwp_text, preprocess_text
from count import analyze_frequency, visualize_frequencies, save_frequencies_to_text, generate_wordcloud
from network import build_cooccurrence_network, visualize_network,compute_centrality_measures
from utils import load_data_from_file, create_output_path,get_special_tokens, load_stopwords_and_mapping, save_frequencies_to_text

def main():
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    base_dir = "/Users/hwangjaewon/Downloads/count_analylsis/"
    recent_data_path = os.path.join(base_dir, "final_data")
    result_base_path = os.path.join(base_dir, "result/result_final")
    stopwords_path = os.path.join(base_dir, "dictionary/stopwords.txt")
    word_mapping_path = os.path.join(base_dir, "dictionary/word_mapping.txt")

    # ë¶ˆìš©ì–´ ë° ë‹¨ì–´ ë§¤í•‘ ë¡œë“œ
    stopwords, word_mapping = load_stopwords_and_mapping(stopwords_path, word_mapping_path)

    # Komoran ì„¤ì •
    komoran = Komoran(userdic=os.path.join(base_dir, "dictionary/custom_dic.txt"))

    logger.info("Starting Analysis!!")

    # í‚¤ì›Œë“œ íŒŒì¼ ì½ê¸° í•¨ìˆ˜
    def load_keywords(file_path):
        """í‚¤ì›Œë“œ íŒŒì¼ì„ ì½ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜, íŒŒì¼ì´ ì—†ì„ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        if not os.path.exists(file_path):
            logger.warning(f"âš  í‚¤ì›Œë“œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return []
        
        with open(file_path, "r", encoding="utf-8") as f:
            keywords = [line.strip() for line in f.readlines()]

        if not keywords:
            logger.warning(f"âš  í‚¤ì›Œë“œ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤: {file_path}")
        
        return keywords
    
    def update_custom_dic(custom_dic_path, new_words):
        """âœ… ë¹ˆë„ìˆ˜ê°€ 0ì¸ í‚¤ì›Œë“œë¥¼ custom_dic.txt í˜•ì‹(NNP)ìœ¼ë¡œ ì¶”ê°€"""
        # ê¸°ì¡´ ì‚¬ìš©ì ì‚¬ì „ ë¡œë“œ
        existing_words = set()
        if os.path.exists(custom_dic_path):
            with open(custom_dic_path, "r", encoding="utf-8") as f:
                existing_words = set(line.strip() for line in f.readlines())

        # ìƒˆë¡œìš´ ë‹¨ì–´ ì¶”ê°€ (NNP íƒœê·¸ ì ìš©)
        updated_words = existing_words.union({f"{word}\tNNP" for word in new_words})
        
        # ì‚¬ìš©ì ì‚¬ì „ ì—…ë°ì´íŠ¸
        with open(custom_dic_path, "w", encoding="utf-8") as f:
            for word in sorted(updated_words):  # ì‚¬ì „ ì •ë ¬ í›„ ì €ì¥
                f.write(f"{word}\n")
        
        logger.info(f"âœ… ì‚¬ìš©ì ì‚¬ì „ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {custom_dic_path}")

    for i in range(1, 9):
        logger.info(f"Processing file: {i}ë²ˆ ë¬¸í•­")

        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        file_name = f"{i}ë²ˆ ë¬¸í•­.hwp"
        file_path = os.path.join(recent_data_path, file_name)
        result_path = os.path.join(result_base_path, f"{i}ë²ˆ")
        os.makedirs(result_path, exist_ok=True)  # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±

        # í‚¤ì›Œë“œ íŒŒì¼ ë¡œë“œ
        keyword_path = os.path.join(base_dir, f"keyword/keyword_{i}.txt")
        keywords = load_keywords(keyword_path)

        # í‚¤ì›Œë“œ íŒŒì¼ì´ ë¹„ì–´ ìˆëŠ” ê²½ìš° ìŠ¤í‚µ
        if not keywords:
            logger.warning(f"âš  {i}ë²ˆ ë¬¸í•­ì˜ í‚¤ì›Œë“œ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. count_freq.txtë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = get_hwp_text(file_path)
        logger.info("âœ… í•œê¸€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”
        tokens = preprocess_text(text, komoran, stopwords)
        logger.info("âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ")

        # ë‹¨ì–´ ë§¤í•‘ ì ìš© (í‚¤ì›Œë“œ ëª©ë¡ì— ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œ)
        mapped_tokens = [word_mapping.get(word, word) for word in tokens if word in keywords]
        # í† í°í™”ëœ ë‹¨ì–´ê°€ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
        if not mapped_tokens:
            logger.warning(f"âš  {i}ë²ˆ ë¬¸í•­ì—ì„œ í‚¤ì›Œë“œì™€ ì¼ì¹˜í•˜ëŠ” ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue  # ë‹¤ìŒ ë¬¸í•­ìœ¼ë¡œ ë„˜ì–´ê°

        # í† í°í™”ëœ ë‹¨ì–´ê°€ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
        if not mapped_tokens:
            logger.warning(f"âš  {i}ë²ˆ ë¬¸í•­ì—ì„œ í† í°í™”ëœ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ë¹ˆë„ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ (í‚¤ì›Œë“œ íŒŒì¼ì— ìˆëŠ” ë‹¨ì–´ë§Œ, ì—†ì„ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •)
        keyword_frequencies = {kw: mapped_tokens.count(kw) if kw in mapped_tokens else 0 for kw in keywords}
        

        # âœ… count_freq.txt ì €ì¥ (í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ë‹¨ì–´ í¬í•¨)
        save_frequencies_to_text(keyword_frequencies, create_output_path(result_path, "count_freq.txt"))

        # í‚¤ì›Œë“œ ë¹ˆë„ í™•ì¸ ë¡œê·¸ ì¶”ê°€
        logger.info(f"ğŸ“Š {i}ë²ˆ ë¬¸í•­ í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì™„ë£Œ")


        # ëª¨ë“  í‚¤ì›Œë“œê°€ 0ì¼ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
        if all(freq == 0 for freq in keyword_frequencies.values()):
            logger.warning(f"âš  {i}ë²ˆ ë¬¸í•­ì—ì„œ ëª¨ë“  í‚¤ì›Œë“œê°€ 0ì…ë‹ˆë‹¤. count_freq.txtë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue  # ë‹¤ìŒ ë¬¸í•­ìœ¼ë¡œ ë„˜ì–´ê°
        
        # í‚¤ì›Œë“œ ë¹ˆë„ í™•ì¸ ë¡œê·¸ ì¶”ê°€
        logger.info(f"ğŸ“Š {i}ë²ˆ ë¬¸í•­ í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ì™„ë£Œ")

        # ë¹ˆë„ ë°ì´í„°ê°€ ì—†ì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
        if not keyword_frequencies or all(freq == 0 for freq in keyword_frequencies.values()):
            logger.warning(f"âš  {i}ë²ˆ ë¬¸í•­ì—ì„œ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. count_freq.txtë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue

        # í‚¤ì›Œë“œ ë¹ˆë„ ë°ì´í„° ì €ì¥
        # save_frequencies_to_text(keyword_frequencies, create_output_path(result_path, "count_freq.txt"))
    
        # ë¹ˆë„ ìƒìœ„ 30ê°œë§Œ ì‹œê°í™”
        top_30_keywords = sorted(keyword_frequencies.items(), key=lambda x: x[1], reverse=True)[:30]
        visualize_frequencies(top_30_keywords, i, top_n=30, output_file=create_output_path(result_path, "count_plot.png"))

        # ë„¤íŠ¸ì›Œí¬ ì—£ì§€ ìƒì„± (í‚¤ì›Œë“œë§Œ í¬í•¨)
        # âœ… ë¹ˆë„ ìƒìœ„ 30ê°œ ë‹¨ì–´ë§Œ ë…¸ë“œë¡œ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ìƒì„±
        top_30_words = [word for word, _ in top_30_keywords]  # ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ

        edges = build_cooccurrence_network(mapped_tokens, stopwords, top_30_words, window_size=2)

        # ë„¤íŠ¸ì›Œí¬ ì—£ì§€ê°€ ì—†ì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
        if not edges:
            logger.warning(f"âš  {i}ë²ˆ ë¬¸í•­ì—ì„œ ë„¤íŠ¸ì›Œí¬ ì—£ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¤‘ì‹¬ì„± ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue  # ë‹¤ìŒ ë¬¸í•­ìœ¼ë¡œ ë„˜ì–´ê°
        # edges = build_cooccurrence_network(mapped_tokens, stopwords, list(keyword_frequencies.keys()), window_size=3)
        
        # ë„¤íŠ¸ì›Œí¬ ì—£ì§€ê°€ ì—†ì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
        if not edges:
            logger.warning(f"âš  {i}ë²ˆ ë¬¸í•­ì—ì„œ ë„¤íŠ¸ì›Œí¬ ì—£ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¤‘ì‹¬ì„± ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # ì¤‘ì‹¬ì„± ë¶„ì„ ê²°ê³¼ ìƒì„±
        centrality_output_path = create_output_path(result_path, "centrality_measures.csv")
        compute_centrality_measures(edges, top_30_words, output_path=centrality_output_path)
        
        # ì¤‘ì‹¬ì„± ë¶„ì„ íŒŒì¼ì´ ë¹„ì–´ ìˆì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
        # ì¤‘ì‹¬ì„± ë¶„ì„ íŒŒì¼ì´ ë¹„ì–´ ìˆì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
        try:
            df_centrality = pd.read_csv(centrality_output_path)
        except pd.errors.EmptyDataError:
            logger.warning(f"âš  ì¤‘ì‹¬ì„± ë¶„ì„ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤: {centrality_output_path}")
            df_centrality = pd.DataFrame(columns=["ë‹¨ì–´", "ì—°ê²° ì¤‘ì‹¬ì„±", "ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±"])

        visualize_network(
                        edges, 
                        threshold=3, 
                        output_file= create_output_path(result_path, "full_network_plot.png"), 
                        uniform_color="lightblue", 
                        uniform_size=150  # ë…¸ë“œ í¬ê¸° ê³ ì •
                    )
        logger.info(f'{i}ë²ˆ full ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ì™„ë£Œ')
    
        # ë°ì´í„° ë³‘í•© ë° ì €ì¥ (í‚¤ì›Œë“œë§Œ í¬í•¨)
        df_frequency = pd.DataFrame(list(keyword_frequencies.items()), columns=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"])
        df_combined = df_frequency.merge(df_centrality, on="ë‹¨ì–´", how="left").fillna(0)
        df_combined.to_csv(create_output_path(result_path, "word_analysis_table.csv"), index=False, encoding="utf-8-sig")

        logger.info(f"âœ… {i}ë²ˆ ë¬¸í•­ ë¶„ì„ ì™„ë£Œ")

    logger.info("ğŸ¯ ëª¨ë“  ë¬¸í•­ ë¶„ì„ ì™„ë£Œ!")

# def main():
#     # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
#     base_dir = "/Users/hwangjaewon/Downloads/count_analylsis/"
#     recent_data_path = os.path.join(base_dir, "final_data")
#     result_base_path = os.path.join(base_dir, "result/result_final")
#     stopwords_path = os.path.join(base_dir, "dictionary/stopwords.txt")
#     word_mapping_path = os.path.join(base_dir, "dictionary/word_mapping.txt")

#     # ë¶ˆìš©ì–´ ë° ë‹¨ì–´ ë§¤í•‘ ë¡œë“œ
#     stopwords, word_mapping=  load_stopwords_and_mapping(stopwords_path, word_mapping_path)
#     # Komoran ì„¤ì •
#     komoran = Komoran(userdic=os.path.join(base_dir, "dictionary/custom_dic.txt"))
#     additional_tokens = ["AI", "ì—†ìŠµë‹ˆë‹¤",'ë””ì§€í„¸ì½˜í…ì¸ ê³¼', 'íƒœì¬ëŒ€í•™êµ','ììœ¨ê·œì œìœ„ì›íšŒ','í…ŒìŠ¤íŠ¸ë² ë“œ', 'ë¡œë¸”ë¡ìŠ¤', 'ë¹„ëŒ€ë©´',
#                          'í˜ì‹ ìœµí•©ëŒ€í•™ì‚¬ì—…', 'ì •ë¶€ì£¼ë„', 'ê²Œì´ë¯¸í”¼ì¼€ì´ì…˜', 'XRì‹¤ì¦ì„¼í„°', 'ì§€ì ì¬ì‚°ê¶Œ', 'ì •ë³´í†µì‹ ê¸°íší‰ê°€ì›', 
#                          'í•´ì™¸ê¸°ìˆ ì—°ìˆ˜', 'IP', 'ë””ì§€í„¸ì½˜í…ì¸ ìœ¡ì„±ì‚¬ì—…', 'ì—°êµ¬ê³¼ì œ', 'ì¤‘ì¬ì•ˆ', 'ê°€ìƒìœµí•©ì‚¬ì—…',
#                          'ê²Œì„ì‚°ì—…ì§„í¥ë²•', 'ì»´íˆ¬ë²„ìŠ¤', 'IoT','NFT', 'íŒ¨ìŠ¤íŠ¸íŠ¸ë™',
#                          'ì´í”„ëœë“œ', 'ë²¤ì²˜ìºí”¼íƒˆ','VC','í•œêµ­ì§€ëŠ¥ì •ë³´ì‚¬íšŒì§„í¥ì›','ì…§ë‹¤ìš´ì œ', 'P2E','IPë°¸ë¥˜ì²´ì¸', 'ë¬¸í™”ì‚°ì—…ê³µì •ìœ í†µë²•',  
#                          'ì¤‘ì†Œê¸°ì—…ë²¤ì²˜ë¶€','ì„œìš¸ê²½ì œì§„í¥ì›', 'ìˆ˜ìµëª¨ë¸', 'ì½”ë¦¬ì•„ë©”íƒ€ë²„ìŠ¤í˜ìŠ¤í‹°ë²Œ', 'ê³µê°„ì»´í“¨íŒ…','ëª¨ë¥´ê² ìŠµë‹ˆë‹¤',
#                          'ì›¹ì†Œì„¤', 'ì¹´í…Œê³ ë¦¬', 'ì§„í¥ì›', 'ë””ì§€í„¸êµê³¼ì„œ', 'ë°¸ë¥˜ì²´ì¸', 'ì•ˆì „ì„±', 'ë¼ì´í”„ìŠ¤íƒ€ì¼', 'ì‚°ì—…ì ', 'ê³µë¡ í™”', 'ê³ ë ¹í™”', 
#                          'ì‚¬ì—…í™”', 'ì‚°ì—…í†µì‚°ìì›ë¶€', 'ê°„ì ‘ìë³¸', 'ë°”ìš°ì²˜', 'ê°€ìƒí™˜ê²½', 'ìˆ˜ìš©ì„±', 'ëŒ€í•™ìƒ', 
#                          'ì‚¬íšŒì ', 'ì‹ ë¢°ì„±', 'ì¸ì¬ì–‘ì„±', 'ëŒ€í•™ì›ìƒ', 'ì„ ìˆœí™˜', 'ì§„í¥ë²•', 'SNS', 'ê¸ˆì „ì ',  'ì‹œë²”ì‚¬ì—…',
#                          'ë¶€ì²˜ê°„', 'ì‚°ì—…ì ', 'ê³ ë ¹í™”', 'ì‹¬ë¦¬ì ', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ', 'ìŠ¤ë§ˆíŠ¸ì‹œí‹°', 'ì˜ˆì™¸ê·œì •', 'íƒ„ì†Œì¤‘ë¦½', 'ì‚¬íšŒì ', 
#                          'ë””ì§€í„¸ì „í™˜','ì‚¬íšŒì ë¬¸ì œ', 'ì§„í¥ë²•', 'ë©”íƒ€ë²„ìŠ¤ìº í¼ìŠ¤', 'ê¸°í›„ë³€í™”', 'ìœ¤ë¦¬ì ', 'ì¸ê³µì§€ëŠ¥', 'ì‹¤ê°í˜•','ì‹ ê¸°ìˆ ',
#                          'ì‚¬í–‰ì„±', 'ë¯¼ê´€í˜‘ë ¥', 'ìœ ê¸°ì ', 'ì˜ˆì™¸ì¡°í•­', 'ë‹¨ê³„ë³„', 'ì§€ì—­íŠ¹êµ¬', 'ë²¤ì²˜ìºí”¼í„¸', 'ì¼ìƒ', 'ì œë„í™”', 'ë°”ìš°ì²˜', 
#                          'ì •ë³´í†µì‹ ì „ëµìœ„ì›íšŒ', 'êµìœ¡ê³„', 'ì˜¤í”ˆì´ë…¸ë² ì´ì…˜', 'ì—°ê³„ì„±', 'ë””ì§€í„¸ì „í™˜', 'ì˜ê²¬ìˆ˜ë ´', 'ê¸ˆì „ì ', 'ì¸ê³µì§€ëŠ¥', 
#                          'í†µí•©ì ', 'ì‹¤íš¨ì„±', 'í•„ìš”ì„±', 'ììœ¨ì„±', 'ìºì¦˜', 'ê²½ì§ì„±',
#                          'í˜„ì‹¤ì„¸ê³„', 'ì‹¤ì¦íŠ¹ë¡€', 'ì¹´í…Œê³ ë¦¬', 'íŒŒê´´ì í˜ì‹ ', 'ëŒ€í•™ì›', 'ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬', 'ì¢…í•©ì ', 'ì¬êµ¬ì¡°í™”', 
#                          'ìœ ê¸°ì ', 'ì˜ˆì™¸ì¡°í•­', 'ëŒ€í•™êµ', 'ê³µë¡ í™”', 'ë²•ì œí™”', 'ì‚¬ë¡€ë°œêµ´', 'ë‹´ë¡ ì˜ì¥', 'ì•ˆì •ì ', 
#                          'ì‚¬ì—…í™”', 'ì¹¼ë¦¬ë²„ìŠ¤', 'í‰ê°€ìœ„ì›íšŒ', 'ë°”ìš°ì²˜', 'êµ­ë°©ë¶€', 'ì˜¤í”ˆì´ë…¸ë² ì´ì…˜', 'ê¸°ìˆ ì ', 'ê³µê³µê¸°ê´€', 
#                          'ì‚¬í–‰ì„±ê·œì œ', 'ì§„í¥ë²•', 'êµ¬ì²´í™”', 'ì¸ê³µì§€ëŠ¥', 'ììœ¨ì„±','ë¹„í™œì„±í™”', 'ì§„í¥ë²•', 'ê³µë¡ í™”', 'ê¸°ìˆ ê°œë°œ', 
#                          'ì½˜í…ì¸ ì§„í¥ë²•', 'ê°€ìƒìœµí•©ì •ì±…', 'ê°€ì´ë“œ', 'ì¸ê³µì§€ëŠ¥', 'ì‚¬ì—…í™”', 'ê¸°íšì¬ì •ë¶€', 'ì„¸ë¶€ì ', 'ì‹ ê¸°ìˆ ']

#     # 1ë²ˆë¶€í„° 6ë²ˆ ë¬¸í•­ ë°˜ë³µ ì²˜ë¦¬
#     logger.info("Starting Analysis!!")
#     # í‚¤ì›Œë“œ íŒŒì¼ ì½ê¸° í•¨ìˆ˜ ì¶”ê°€
#     def load_keywords(file_path):
#         """í‚¤ì›Œë“œ íŒŒì¼ì„ ì½ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜, íŒŒì¼ì´ ì—†ì„ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
#         if not os.path.exists(file_path):
#             logger.warning(f"âš  í‚¤ì›Œë“œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
#             return []
        
#         with open(file_path, "r", encoding="utf-8") as f:
#             keywords = [line.strip() for line in f.readlines()]

#         if not keywords:
#             logger.warning(f"âš  í‚¤ì›Œë“œ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤: {file_path}")
        
#         return keywords

#     for i in range(1, 9):
#         logger.info(f"Processing file: {i}ë²ˆ ë¬¸í•­")
#         file_name = f"{i}ë²ˆ ë¬¸í•­.hwp"
#         file_path = os.path.join(recent_data_path, file_name)
#         result_path = os.path.join(result_base_path, f"{i}ë²ˆ")
        
#         keyword_path = os.path.join(base_dir, f"keyword/keyword_{i}.txt")
#         keywords = load_keywords(keyword_path) 


#         # í…ìŠ¤íŠ¸ ì¶”ì¶œ
#         text = get_hwp_text(file_path)

#         # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”
#         tokens = preprocess_text(text, komoran, stopwords, additional_tokens)

#         # ë‹¨ì–´ ë§¤í•‘ ì ìš© (í‚¤ì›Œë“œ ëª©ë¡ì— ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œ)
#         mapped_tokens = [word_mapping.get(word, word) for word in tokens if word in keywords]

#         # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ (í‚¤ì›Œë“œì— í¬í•¨ëœ ë‹¨ì–´ë§Œ)
#         keyword_frequencies = {kw: mapped_tokens.count(kw) for kw in keywords}

#         # í‚¤ì›Œë“œê°€ í•˜ë‚˜ë„ ì—†ì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
#         if not keyword_frequencies:
#             logger.warning(f"âš  {i}ë²ˆ ë¬¸í•­ì—ì„œ í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
#             continue  # ë‹¤ìŒ ë¬¸í•­ìœ¼ë¡œ ë„˜ì–´ê°

#         # í‚¤ì›Œë“œ ë¹ˆë„ ë°ì´í„° ì €ì¥
#         save_frequencies_to_text(keyword_frequencies, create_output_path(result_path, "count_freq.txt"))

#         # ë¹ˆë„ ìƒìœ„ 30ê°œë§Œ ì‹œê°í™”
#         top_30_keywords = sorted(keyword_frequencies.items(), key=lambda x: x[1], reverse=True)[:30]
#         visualize_frequencies(top_30_keywords, i, top_n=30, output_file=create_output_path(result_path, "count_plot.png"))

#         # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
#         os.makedirs(result_path, exist_ok=True)

#         # ë„¤íŠ¸ì›Œí¬ ì—£ì§€ ìƒì„±
#         edges = build_cooccurrence_network(mapped_tokens, stopwords, list(keyword_frequencies.items())[:100], window_size=3)

#         # ë„¤íŠ¸ì›Œí¬ ì—£ì§€ê°€ ì—†ì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
#         if not edges:
#             logger.warning(f"âš  {i}ë²ˆ ë¬¸í•­ì—ì„œ ë„¤íŠ¸ì›Œí¬ ì—£ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¤‘ì‹¬ì„± ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
#             continue  # ë‹¤ìŒ ë¬¸í•­ìœ¼ë¡œ ë„˜ì–´ê°

#         # ì¤‘ì‹¬ì„± ë¶„ì„ ê²°ê³¼ ìƒì„±
#         centrality_output_path = create_output_path(result_path, "centrality_measures.csv")
#         compute_centrality_measures(edges, keyword_frequencies, output_path=centrality_output_path)

#         # ì¤‘ì‹¬ì„± ë¶„ì„ íŒŒì¼ì´ ë¹„ì–´ ìˆì„ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
#         try:
#             df_centrality = pd.read_csv(centrality_output_path)
#         except pd.errors.EmptyDataError:
#             logger.warning(f"âš  ì¤‘ì‹¬ì„± ë¶„ì„ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤: {centrality_output_path}")
#             df_centrality = pd.DataFrame(columns=["ë‹¨ì–´", "ì—°ê²° ì¤‘ì‹¬ì„±", "ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±"])  # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±

#         # ë°ì´í„° ë³‘í•© ë° ì €ì¥ (í‚¤ì›Œë“œë§Œ í¬í•¨)
#         df_frequency = pd.DataFrame(list(keyword_frequencies.items()), columns=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"])
#         df_combined = df_frequency.merge(df_centrality, on="ë‹¨ì–´", how="left")
#         df_combined.to_csv(create_output_path(result_path, "word_analysis_table.csv"), index=False, encoding="utf-8-sig")

#         logger.info(f"Completed processing for {i}ë²ˆ ë¬¸í•­")
        
        
#     logger.info("Processing complete for all files.")

if __name__ == "__main__":
    main()