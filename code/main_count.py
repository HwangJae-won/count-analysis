import os, sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from logger import logger
from konlpy.tag import Komoran
import pandas as pd
from process import get_hwp_text, preprocess_text
from count import analyze_frequency, visualize_frequencies, save_frequencies_to_text, generate_wordcloud
from network import build_cooccurrence_network, visualize_network, compute_centrality_measures
from utils import load_data_from_file, create_output_path, get_special_tokens, load_stopwords_and_mapping, save_frequencies_to_text

def update_custom_dic(custom_dic_path, new_words):
    """âœ… ë¹ˆë„ìˆ˜ê°€ 0ì¸ í‚¤ì›Œë“œë¥¼ custom_dic.txt í˜•ì‹(NNP)ìœ¼ë¡œ ì¶”ê°€"""
    existing_words = set()
    if os.path.exists(custom_dic_path):
        with open(custom_dic_path, "r", encoding="utf-8") as f:
            existing_words = set(line.strip() for line in f.readlines())

    updated_words = existing_words.union({f"{word}\tNNP" for word in new_words})
    
    with open(custom_dic_path, "w", encoding="utf-8") as f:
        for word in sorted(updated_words):
            f.write(f"{word}\n")
    
    logger.info(f"âœ… ì‚¬ìš©ì ì‚¬ì „ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {custom_dic_path}")

def main():
    base_dir = "/Users/hwangjaewon/Downloads/count_analylsis/"
    recent_data_path = os.path.join(base_dir, "final_data")
    result_base_path = os.path.join(base_dir, "result/result_final")
    stopwords_path = os.path.join(base_dir, "dictionary/stopwords.txt")
    word_mapping_path = os.path.join(base_dir, "dictionary/word_mapping.txt")
    custom_dic_path = os.path.join(base_dir, "dictionary/custom_dic.txt")
    additional_tokens = ['ì‹ ê¸°ìˆ ', "AI", "ì—†ìŠµë‹ˆë‹¤",'ë””ì§€í„¸ì½˜í…ì¸ ê³¼', 'íƒœì¬ëŒ€í•™êµ','ììœ¨ê·œì œìœ„ì›íšŒ','í…ŒìŠ¤íŠ¸ë² ë“œ', 'ë¡œë¸”ë¡ìŠ¤', 'ë¹„ëŒ€ë©´',
                        'í˜ì‹ ìœµí•©ëŒ€í•™ì‚¬ì—…', 'ì •ë¶€ì£¼ë„', 'ê²Œì´ë¯¸í”¼ì¼€ì´ì…˜', 'XRì‹¤ì¦ì„¼í„°', 'ì§€ì ì¬ì‚°ê¶Œ', 'ì •ë³´í†µì‹ ê¸°íší‰ê°€ì›', 
                        'í•´ì™¸ê¸°ìˆ ì—°ìˆ˜', 'IP', 'ë””ì§€í„¸ì½˜í…ì¸ ìœ¡ì„±ì‚¬ì—…', 'ì—°êµ¬ê³¼ì œ', 'ì¤‘ì¬ì•ˆ', 'ê°€ìƒìœµí•©ì‚¬ì—…',
                        'ê²Œì„ì‚°ì—…ì§„í¥ë²•', 'ì»´íˆ¬ë²„ìŠ¤', 'IoT','NFT', 'íŒ¨ìŠ¤íŠ¸íŠ¸ë™',
                        'ì´í”„ëœë“œ', 'ë²¤ì²˜ìºí”¼íƒˆ','VC','í•œêµ­ì§€ëŠ¥ì •ë³´ì‚¬íšŒì§„í¥ì›','ì…§ë‹¤ìš´ì œ', 'P2E','IPë°¸ë¥˜ì²´ì¸', 'ë¬¸í™”ì‚°ì—…ê³µì •ìœ í†µë²•',  
                        'ì¤‘ì†Œê¸°ì—…ë²¤ì²˜ë¶€','ì„œìš¸ê²½ì œì§„í¥ì›', 'ìˆ˜ìµëª¨ë¸', 'ì½”ë¦¬ì•„ë©”íƒ€ë²„ìŠ¤í˜ìŠ¤í‹°ë²Œ', 'ê³µê°„ì»´í“¨íŒ…','ëª¨ë¥´ê² ìŠµë‹ˆë‹¤',
                        'ì›¹ì†Œì„¤', 'ì¹´í…Œê³ ë¦¬', 'ì§„í¥ì›', 'ë””ì§€í„¸êµê³¼ì„œ', 'ë°¸ë¥˜ì²´ì¸', 'ì•ˆì „ì„±', 'ë¼ì´í”„ìŠ¤íƒ€ì¼', 'ì‚°ì—…ì ', 'ê³µë¡ í™”', 'ê³ ë ¹í™”', 
                        'ì‚¬ì—…í™”', 'ì‚°ì—…í†µì‚°ìì›ë¶€', 'ê°„ì ‘ìë³¸', 'ë°”ìš°ì²˜', 'ê°€ìƒí™˜ê²½', 'ìˆ˜ìš©ì„±', 'ëŒ€í•™ìƒ', 
                        'ì‚¬íšŒì ', 'ì‹ ë¢°ì„±', 'ì¸ì¬ì–‘ì„±', 'ëŒ€í•™ì›ìƒ', 'ì„ ìˆœí™˜', 'ì§„í¥ë²•', 'SNS', 'ê¸ˆì „ì ',  'ì‹œë²”ì‚¬ì—…',
                        'ë¶€ì²˜ê°„', 'ì‚°ì—…ì ', 'ê³ ë ¹í™”', 'ì‹¬ë¦¬ì ', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ', 'ìŠ¤ë§ˆíŠ¸ì‹œí‹°', 'ì˜ˆì™¸ê·œì •', 'íƒ„ì†Œì¤‘ë¦½', 'ì‚¬íšŒì ', 
                        'ë””ì§€í„¸ì „í™˜','ì‚¬íšŒì ë¬¸ì œ', 'ì§„í¥ë²•', 'ë©”íƒ€ë²„ìŠ¤ìº í¼ìŠ¤', 'ê¸°í›„ë³€í™”', 'ìœ¤ë¦¬ì ', 'ì¸ê³µì§€ëŠ¥', 'ì‹¤ê°í˜•','ì‹ ê¸°ìˆ ',
                        'ì‚¬í–‰ì„±', 'ë¯¼ê´€í˜‘ë ¥', 'ìœ ê¸°ì ', 'ì˜ˆì™¸ì¡°í•­', 'ë‹¨ê³„ë³„', 'ì§€ì—­íŠ¹êµ¬', 'ë²¤ì²˜ìºí”¼í„¸', 'ì¼ìƒ', 'ì œë„í™”', 'ë°”ìš°ì²˜', 
                        'ì •ë³´í†µì‹ ì „ëµìœ„ì›íšŒ', 'êµìœ¡ê³„', 'ì˜¤í”ˆì´ë…¸ë² ì´ì…˜', 'ì—°ê³„ì„±', 'ë””ì§€í„¸ì „í™˜', 'ì˜ê²¬ìˆ˜ë ´', 'ê¸ˆì „ì ', 'ì¸ê³µì§€ëŠ¥', 
                        'í†µí•©ì ', 'ì‹¤íš¨ì„±', 'í•„ìš”ì„±', 'ììœ¨ì„±', 'ìºì¦˜', 'ê²½ì§ì„±',
                        'í˜„ì‹¤ì„¸ê³„', 'ì‹¤ì¦íŠ¹ë¡€', 'ì¹´í…Œê³ ë¦¬', 'íŒŒê´´ì í˜ì‹ ', 'ëŒ€í•™ì›', 'ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬', 'ì¢…í•©ì ', 'ì¬êµ¬ì¡°í™”', 
                        'ìœ ê¸°ì ', 'ì˜ˆì™¸ì¡°í•­', 'ëŒ€í•™êµ', 'ê³µë¡ í™”', 'ë²•ì œí™”', 'ì‚¬ë¡€ë°œêµ´', 'ë‹´ë¡ ì˜ì¥', 'ì•ˆì •ì ', 
                        'ì‚¬ì—…í™”', 'ì¹¼ë¦¬ë²„ìŠ¤', 'í‰ê°€ìœ„ì›íšŒ', 'ë°”ìš°ì²˜', 'êµ­ë°©ë¶€', 'ì˜¤í”ˆì´ë…¸ë² ì´ì…˜', 'ê¸°ìˆ ì ', 'ê³µê³µê¸°ê´€', 
                        'ì‚¬í–‰ì„±ê·œì œ', 'ì§„í¥ë²•', 'êµ¬ì²´í™”', 'ì¸ê³µì§€ëŠ¥', 'ììœ¨ì„±','ë¹„í™œì„±í™”', 'ì§„í¥ë²•', 'ê³µë¡ í™”', 'ê¸°ìˆ ê°œë°œ', 
                        'ì½˜í…ì¸ ì§„í¥ë²•', 'ê°€ìƒìœµí•©ì •ì±…', 'ê°€ì´ë“œ', 'ì¸ê³µì§€ëŠ¥', 'ì‚¬ì—…í™”', 'ê¸°íšì¬ì •ë¶€', 'ì„¸ë¶€ì ']

    stopwords, word_mapping = load_stopwords_and_mapping(stopwords_path, word_mapping_path)
    komoran = Komoran(userdic=custom_dic_path)

    logger.info("Starting Analysis!!")
    
    def load_keywords(file_path):
        if not os.path.exists(file_path):
            logger.warning(f"âš  í‚¤ì›Œë“œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return []
        with open(file_path, "r", encoding="utf-8") as f:
            return [line.strip() for line in f.readlines()]
    
    for i in range(1, 9):
        logger.info(f"Processing file: {i}ë²ˆ ë¬¸í•­")
        file_name = f"{i}ë²ˆ ë¬¸í•­.hwp"
        file_path = os.path.join(recent_data_path, file_name)
        result_path = os.path.join(result_base_path, f"{i}ë²ˆ")
        os.makedirs(result_path, exist_ok=True)
        
        keyword_path = os.path.join(base_dir, f"keyword/keyword_{i}.txt")
        keywords = load_keywords(keyword_path)
        
        if not keywords:
            logger.warning(f"âš  {i}ë²ˆ ë¬¸í•­ì˜ í‚¤ì›Œë“œ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. count_freq.txtë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue

        text = get_hwp_text(file_path)
        tokens = preprocess_text(text, komoran, stopwords, additional_tokens)
        mapped_tokens = [word_mapping.get(word, word) for word in tokens]

        keyword_frequencies = {kw: mapped_tokens.count(kw) for kw in keywords}
        save_frequencies_to_text(
            dict(sorted(keyword_frequencies.items(), key=lambda item: item[1], reverse=True)),
            create_output_path(result_path, "count_freq.txt")
            )
        
        zero_freq_keywords = {kw for kw, freq in keyword_frequencies.items() if freq == 0}
        update_custom_dic(custom_dic_path, zero_freq_keywords)
        komoran = Komoran(userdic=custom_dic_path)

        tokens = preprocess_text(text, komoran, stopwords, additional_tokens)
        mapped_tokens = [word_mapping.get(word, word) for word in tokens]
        keyword_frequencies = {kw: mapped_tokens.count(kw) for kw in keywords}
        save_frequencies_to_text(
            dict(sorted(keyword_frequencies.items(), key=lambda item: item[1], reverse=True)),
            create_output_path(result_path, "count_freq.txt")
            )
        
        visualize_frequencies(sorted(keyword_frequencies.items(), key=lambda x: x[1], reverse=True), i, output_file=create_output_path(result_path, "count_plot.png"))
        
        words_for_network = list(keyword_frequencies.keys())
        edges = build_cooccurrence_network(mapped_tokens, stopwords, words_for_network, window_size=2)
        
        if not edges:
            logger.warning(f"âš  {i}ë²ˆ ë¬¸í•­ì—ì„œ ë„¤íŠ¸ì›Œí¬ ì—£ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¤‘ì‹¬ì„± ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        centrality_output_path = create_output_path(result_path, "centrality_measures.csv")
        compute_centrality_measures(edges, words_for_network, output_path=centrality_output_path)
        
        try:
            df_centrality = pd.read_csv(centrality_output_path).sort_values(by=['ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±'], ascending=False)
            df_centrality.to_csv(centrality_output_path, index=False, encoding='utf-8-sig', float_format="%.3f")
        except pd.errors.EmptyDataError:
            logger.warning(f"âš  ì¤‘ì‹¬ì„± ë¶„ì„ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤: {centrality_output_path}")
            df_centrality = pd.DataFrame(columns=["ë‹¨ì–´", "ì—°ê²° ì¤‘ì‹¬ì„±", "ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±"])
        
        visualize_network(edges, threshold=2, output_file=create_output_path(result_path, "full_network_plot.png"), uniform_color="lightblue", uniform_size=150)
        
        df_frequency = pd.DataFrame(list(keyword_frequencies.items()), columns=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"])
        df_combined = df_frequency.merge(df_centrality, on="ë‹¨ì–´", how="left").fillna(0).sort_values(by='ë¹ˆë„ìˆ˜', ascending=False)
        df_combined.to_csv(create_output_path(result_path, "word_analysis_table.csv"), index=False, encoding="utf-8-sig", float_format="%.3f")
        
        logger.info(f"âœ… {i}ë²ˆ ë¬¸í•­ ë¶„ì„ ì™„ë£Œ")
    
    logger.info("ğŸ¯ ëª¨ë“  ë¬¸í•­ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
