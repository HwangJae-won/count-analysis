import os, sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from logger import logger
from konlpy.tag import Komoran
from process import get_hwp_text, preprocess_text
from count import analyze_frequency, visualize_frequencies, save_frequencies_to_text, generate_wordcloud
from network import build_cooccurrence_network, visualize_network,compute_centrality_measures
from utils import load_data_from_file, create_output_path,get_special_tokens, load_stopwords_and_mapping
from emotion import analyze_sentiment_with_context, extract_context_sentences


def main():
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    base_dir = "/Users/hwangjaewon/Downloads/count_analylsis/"
    recent_data_path = os.path.join(base_dir, "recent_data")
    result_base_path = os.path.join(base_dir, "result_v3")
    stopwords_path = os.path.join(base_dir, "stopwords.txt")
    word_mapping_path = os.path.join(base_dir, "word_mapping.txt")

    # ë¶ˆìš©ì–´ ë° ë‹¨ì–´ ë§¤í•‘ ë¡œë“œ
    stopwords, word_mapping=  load_stopwords_and_mapping(stopwords_path, word_mapping_path)
    # Komoran ì„¤ì •
    komoran = Komoran(userdic=os.path.join(base_dir, "custom_dic.txt"))
    additional_tokens = ["AI", "ì—†ìŠµë‹ˆë‹¤",'ë””ì§€í„¸ì½˜í…ì¸ ê³¼', 'íƒœì¬ëŒ€í•™êµ','ììœ¨ê·œì œìœ„ì›íšŒ','í…ŒìŠ¤íŠ¸ë² ë“œ', 'ë¡œë¸”ë¡ìŠ¤', 'ë¹„ëŒ€ë©´',
                         'í˜ì‹ ìœµí•©ëŒ€í•™ì‚¬ì—…', 'ì •ë¶€ì£¼ë„', 'ê²Œì´ë¯¸í”¼ì¼€ì´ì…˜', 'XRì‹¤ì¦ì„¼í„°', 'ì§€ì ì¬ì‚°ê¶Œ', 'ì •ë³´í†µì‹ ê¸°íší‰ê°€ì›', 
                         'í•´ì™¸ê¸°ìˆ ì—°ìˆ˜', 'IP', 'ë””ì§€í„¸ì½˜í…ì¸ ìœ¡ì„±ì‚¬ì—…', 'ì—°êµ¬ê³¼ì œ', 'ì¤‘ì¬ì•ˆ', 'ê°€ìƒìœµí•©ì‚¬ì—…',
                         'ê²Œì„ì‚°ì—…ì§„í¥ë²•', 'ì»´íˆ¬ë²„ìŠ¤', 'IoT','NFT', 'íŒ¨ìŠ¤íŠ¸íŠ¸ë™',
                         'ì´í”„ëœë“œ', 'ë²¤ì²˜ìºí”¼íƒˆ','VC','í•œêµ­ì§€ëŠ¥ì •ë³´ì‚¬íšŒì§„í¥ì›','ì…§ë‹¤ìš´ì œ', 'P2E','IPë°¸ë¥˜ì²´ì¸', 'ë¬¸í™”ì‚°ì—…ê³µì •ìœ í†µë²•',  
                         'ì¤‘ì†Œê¸°ì—…ë²¤ì²˜ë¶€','ì„œìš¸ê²½ì œì§„í¥ì›', 'ìˆ˜ìµëª¨ë¸', 'ì½”ë¦¬ì•„ë©”íƒ€ë²„ìŠ¤í˜ìŠ¤í‹°ë²Œ', 'ê³µê°„ì»´í“¨íŒ…','ëª¨ë¥´ê² ìŠµë‹ˆë‹¤',
                         'ì›¹ì†Œì„¤', 'ì¹´í…Œê³ ë¦¬', 'ì§„í¥ì›', 'ë””ì§€í„¸êµê³¼ì„œ', 'ë°¸ë¥˜ì²´ì¸', 'ì•ˆì „ì„±', 'ë¼ì´í”„ìŠ¤íƒ€ì¼', 'ì‚°ì—…ì ', 'ê³µë¡ í™”', 'ê³ ë ¹í™”', 
                         'ì‚¬ì—…í™”', 'ì‚°ì—…í†µì‚°ìì›ë¶€', 'ê°„ì ‘ìë³¸', 'ë°”ìš°ì²˜', 'ê°€ìƒí™˜ê²½', 'ìˆ˜ìš©ì„±', 'ëŒ€í•™ìƒ', 
                         'ì‚¬íšŒì ', 'ì‹ ë¢°ì„±', 'ì¸ì¬ì–‘ì„±', 'ëŒ€í•™ì›ìƒ', 'ì„ ìˆœí™˜', 'ì§„í¥ë²•', 'SNS', 'ê¸ˆì „ì ',  'ì‹œë²”ì‚¬ì—…',
                         'ë¶€ì²˜ê°„', 'ì‚°ì—…ì ', 'ê³ ë ¹í™”', 'ì‹¬ë¦¬ì ', 'êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ', 'ìŠ¤ë§ˆíŠ¸ì‹œí‹°', 'ì˜ˆì™¸ê·œì •', 'íƒ„ì†Œì¤‘ë¦½', 'ì‚¬íšŒì ', 
                         'ë””ì§€í„¸ì „í™˜','ì‚¬íšŒì ë¬¸ì œ', 'ì§„í¥ë²•', 'ë©”íƒ€ë²„ìŠ¤ìº í¼ìŠ¤', 'ê¸°í›„ë³€í™”', 'ìœ¤ë¦¬ì ', 'ì¸ê³µì§€ëŠ¥', 'ì‹¤ê°í˜•','ì‹ ê¸°ìˆ ',
                         'ì‚¬í–‰ì„±', 'ë¯¼ê´€í˜‘ë ¥', 'ìœ ê¸°ì ', 'ì˜ˆì™¸ì¡°í•­', 'ë‹¨ê³„ë³„', 'ì§€ì—­íŠ¹êµ¬', 'ë²¤ì²˜ìºí”¼í„¸', 'ì¼ìƒ', 'ì œë„í™”', 'ë°”ìš°ì²˜', 
                         'ì •ë³´í†µì‹ ì „ëµìœ„ì›íšŒ', 'êµìœ¡ê³„', 'ì˜¤í”ˆì´ë…¸ë² ì´ì…˜', 'ì—°ê³„ì„±', 'ë””ì§€í„¸ì „í™˜', 'ì˜ê²¬ìˆ˜ë ´', 'ê¸ˆì „ì ', 'ì¸ê³µì§€ëŠ¥', 
                         'í†µí•©ì ', 'ì‹¤íš¨ì„±', 'í•„ìš”ì„±', 'ììœ¨ì„±', 'ìºì¦˜', 'ê²½ì§ì„±',
                         'í˜„ì‹¤ì„¸ê³„', 'ì‹¤ì¦íŠ¹ë¡€', 'ì¹´í…Œê³ ë¦¬', 'íŒŒê´´ì í˜ì‹ ', 'ëŒ€í•™ì›', 'ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬', 'ì¢…í•©ì ', 'ì¬êµ¬ì¡°í™”', 
                         'ìœ ê¸°ì ', 'ì˜ˆì™¸ì¡°í•­', 'ëŒ€í•™êµ', 'ê³µë¡ í™”', 'ë²•ì œí™”', 'ì‚¬ë¡€ë°œêµ´', 'ë‹´ë¡ ì˜ì¥', 'ì•ˆì •ì ', 
                         'ì‚¬ì—…í™”', 'ì¹¼ë¦¬ë²„ìŠ¤', 'í‰ê°€ìœ„ì›íšŒ', 'ë°”ìš°ì²˜', 'êµ­ë°©ë¶€', 'ì˜¤í”ˆì´ë…¸ë² ì´ì…˜', 'ê¸°ìˆ ì ', 'ê³µê³µê¸°ê´€', 
                         'ì‚¬í–‰ì„±ê·œì œ', 'ì§„í¥ë²•', 'êµ¬ì²´í™”', 'ì¸ê³µì§€ëŠ¥', 'ììœ¨ì„±','ë¹„í™œì„±í™”', 'ì§„í¥ë²•', 'ê³µë¡ í™”', 'ê¸°ìˆ ê°œë°œ', 
                         'ì½˜í…ì¸ ì§„í¥ë²•', 'ê°€ìƒìœµí•©ì •ì±…', 'ê°€ì´ë“œ', 'ì¸ê³µì§€ëŠ¥', 'ì‚¬ì—…í™”', 'ê¸°íšì¬ì •ë¶€', 'ì„¸ë¶€ì ', 'ì‹ ê¸°ìˆ ']

    # 1ë²ˆë¶€í„° 6ë²ˆ ë¬¸í•­ ë°˜ë³µ ì²˜ë¦¬
    logger.info("Starting Analysis!!")
    
    for i in range(1, 9):
        logger.info(f"Processing file: {i}ë²ˆ ë¬¸í•­")
        file_name = f"{i}ë²ˆ ë¬¸í•­.hwp"
        file_path = os.path.join(recent_data_path, file_name)
        result_path = os.path.join(result_base_path, f"{i}ë²ˆ")


        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = get_hwp_text(file_path)

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í°í™”
        tokens = preprocess_text(text, komoran, stopwords, additional_tokens)

        # ë‹¨ì–´ ë§¤í•‘ ì ìš©
        mapped_tokens = [word_mapping.get(word, word) for word in tokens]

        # ë¹ˆë„ ë¶„ì„
        frequencies = analyze_frequency(mapped_tokens)

        # # ì‹œê°í™” ë° ê²°ê³¼ ì €ì¥
        # visualize_frequencies(frequencies, i, top_n=30, output_file=create_output_path(result_path, "count_plot.png"))
        # save_frequencies_to_text(frequencies, create_output_path(result_path, "count_freq.txt"))
        # # generate_wordcloud(frequencies, font_path="/Library/Fonts/AppleGothic.ttf", output_file=create_output_path(result_path, "wordcloud.png"))

        
        # # ë¹ˆë„ ìƒìœ„ 100ê°œ ë‹¨ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ìƒì„±
        # edges = build_cooccurrence_network(tokens, stopwords, frequencies, window_size=3)

        # compute_centrality_measures(edges, frequencies, 
        #                             output_path=create_output_path(result_path, "centrality_measures.csv"))
    
        # # # ìµœë¹ˆ ë‹¨ì–´ 3ê°œ ê°€ì ¸ì˜¤ê¸°
        # # top_keywords = [word for word, freq in frequencies[:3]]
        # # logger.info(f"ìµœë¹ˆ ë‹¨ì–´ 3ê°œ: {top_keywords}")

        # # ì „ì²´ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ì‹œê°í™”
        # visualize_network(
        #                 edges, 
        #                 threshold=3, 
        #                 output_file= create_output_path(result_path, "full_network_plot.png"), 
        #                 uniform_color="lightblue", 
        #                 uniform_size=150  # ë…¸ë“œ í¬ê¸° ê³ ì •
        #             )
        # logger.info(f'{i}ë²ˆ full ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ì™„ë£Œ')
        # ê°ì • ë¶„ì„ ìˆ˜í–‰
        # 3. ë¬¸ë§¥ ìë™ ì¶”ì¶œ
        keywords = [word for word, _ in frequencies[:100]]
        context_sentences = extract_context_sentences(text, keywords)

        # ğŸ“Œ ë¬¸ë§¥ í™•ì¸ (ë””ë²„ê¹…)
        logger.info(f"ğŸ” ì¶”ì¶œëœ ë¬¸ë§¥ ì˜ˆì‹œ: {list(context_sentences.items())[:5]}")

        # 4. ê°ì • ë¶„ì„ ìˆ˜í–‰
        output_path = create_output_path(result_path,"sentiment_analysis.csv")

        analyze_sentiment_with_context(
            frequencies, 
            context_sentences, 
            output_path=output_path
        )

        logger.info("âœ… í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ: ê°ì • ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        logger.info(f'{i}ë²ˆ ê°ì •ë¶„ì„ ì™„ë£Œ')

        # # ìµœë¹ˆ ë‹¨ì–´ë³„ í•„í„°ë§ ë° ê·¸ë˜í”„ ìƒì„±
        # for keyword in top_keywords:
        #     # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì—£ì§€ë§Œ í•„í„°ë§
        #     filtered_edges = [
        #         edge for edge in edges if keyword in edge
        #     ]
            
        #     # ê·¸ë˜í”„ ì €ì¥
        #     output_file = create_output_path(result_path, f"{keyword}_network.png")
        #     visualize_network(
        #                 edges, 
        #                 threshold=1, 
        #                 output_file=output_file, 
        #                 keyword=keyword, 
        #                 size_scale=500  # ì¤‘ì‹¬ì„±ì— ë”°ë¥¸ í¬ê¸° ìŠ¤ì¼€ì¼ ì¡°ì •
        #             )
        #     logger.info(f'{i}ë²ˆ {keyword} ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” ì™„ë£Œ')

        logger.info(f"Completed processing for {i}ë²ˆ ë¬¸í•­")
        
        
    logger.info("Processing complete for all files.")

if __name__ == "__main__":
    main()